{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c11aa7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.orig_data_processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bd43206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thermal Runaway Detection Pipeline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, auc\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import joblib\n",
    "\n",
    "#Configurations\n",
    "DATA_DIR = \"./data\"\n",
    "RESULTS_DIR = \"./results\"\n",
    "WINDOW_SIZE = 100\n",
    "STRIDE = 1\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "#Loading Data\n",
    "def load_all_data(data_dir, window_size, stride):\n",
    "    X_list, y_list = [], []\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        label = 0 if 'healthy' in root.lower() else 1\n",
    "        for fname in files:\n",
    "            if fname.endswith(\".csv\"):\n",
    "                df = pd.read_csv(os.path.join(root, fname))\n",
    "                if df.isnull().values.any() or len(df) < window_size:\n",
    "                    continue\n",
    "                arr = df.values.astype(np.float32)\n",
    "                for i in range(0, len(arr) - window_size + 1, stride):\n",
    "                    X_list.append(arr[i:i + window_size])\n",
    "                    y_list.append(label)\n",
    "    X = np.array(X_list)\n",
    "    y = np.array(y_list)\n",
    "    print(f\"Loaded: X.shape={X.shape}, y.shape={y.shape}\")\n",
    "    return X, y\n",
    "\n",
    "#Scaling\n",
    "def prepare_ml_data(X, y):\n",
    "    n, T, F = X.shape\n",
    "    X_flat = X.reshape(n, T * F)\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X_flat, y, test_size=0.2, stratify=y, random_state=42)\n",
    "    scaler = RobustScaler()\n",
    "    return scaler.fit_transform(X_tr), scaler.transform(X_te), y_tr, y_te\n",
    "\n",
    "#Wrapper for Keras Classifier\n",
    "class MyKerasClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, build_fn, batch_size=32, epochs=10, verbose=0):\n",
    "        self.build_fn = build_fn\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.verbose = verbose\n",
    "        self.model_ = None\n",
    "        self._build_params = {}\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        keras_keys = [\"batch_size\", \"epochs\", \"verbose\", \"build_fn\"]\n",
    "        for key in list(params.keys()):\n",
    "            if key not in keras_keys:\n",
    "                self._build_params[key] = params.pop(key)\n",
    "        for k in keras_keys:\n",
    "            setattr(self, k, params.get(k, getattr(self, k, None)))\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            \"build_fn\": self.build_fn,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"epochs\": self.epochs,\n",
    "            \"verbose\": self.verbose,\n",
    "            **self._build_params\n",
    "        }\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        features = X.shape[-1]\n",
    "        self.model_ = self.build_fn(features=features, **self._build_params)\n",
    "        self.model_.fit(X, y, epochs=self.epochs, batch_size=self.batch_size,\n",
    "                        verbose=self.verbose, callbacks=[EarlyStopping(patience=2)])\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.model_.predict(X) > 0.5).astype(\"int32\")\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.model_.predict(X)\n",
    "\n",
    "#Deep Learning Models\n",
    "def build_lstm(units=64, learning_rate=0.001, features=2):\n",
    "    model = Sequential([\n",
    "        LSTM(units, input_shape=(WINDOW_SIZE, features)),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_cnn(filters=32, kernel_size=3, learning_rate=0.001, features=2):\n",
    "    model = Sequential([\n",
    "        Conv1D(filters, kernel_size, activation='relu', input_shape=(WINDOW_SIZE, features)),\n",
    "        MaxPooling1D(2),\n",
    "        Flatten(),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#Plotting\n",
    "def save_all_confusion_matrices(conf_mats):\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(18, 15))\n",
    "    axs = axs.flatten()\n",
    "    for ax, (name, cm) in zip(axs, conf_mats.items()):\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=[\"Healthy\", \"Runaway\"],\n",
    "                    yticklabels=[\"Healthy\", \"Runaway\"], ax=ax)\n",
    "        ax.set_title(f\"Confusion: {name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, \"all_confusion_matrices.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def save_all_roc_curves(roc_data):\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    for name, (y_true, y_prob) in sorted(roc_data.items()):\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "        auc_val = auc(fpr, tpr)\n",
    "        ax.plot(fpr, tpr, label=f\"{name} (AUC={auc_val:.3f})\")\n",
    "    ax.plot([0, 1], [0, 1], '--', color='gray')\n",
    "    ax.set_title(\"ROC Curves\")\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, \"all_roc_curves_sorted.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def save_auc_comparison(train_aucs, test_aucs):\n",
    "    labels = list(train_aucs.keys())\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.35\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.bar(x - width/2, [train_aucs[l] for l in labels], width, label=\"Train AUC\")\n",
    "    plt.bar(x + width/2, [test_aucs[l] for l in labels], width, label=\"Test AUC\")\n",
    "    plt.xticks(x, labels, rotation=45)\n",
    "    plt.ylabel(\"AUC\")\n",
    "    plt.title(\"Train vs Test AUC Comparison\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, \"train_vs_test_auc.png\"))\n",
    "    plt.close()\n",
    "\n",
    "#Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    X, y = load_all_data(DATA_DIR, WINDOW_SIZE, STRIDE)\n",
    "    _, _, FEATURES = X.shape\n",
    "    X_tr_s, X_te_s, y_tr, y_te = prepare_ml_data(X, y)\n",
    "    X_tr_dl, X_te_dl, y_tr_dl, y_te_dl = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "    summary, confusion_matrices, roc_curves = [], {}, {}\n",
    "    train_auc, test_auc = {}, {}\n",
    "    best_params_all = {}\n",
    "\n",
    "    #Classical Models with GridSearchCV\n",
    "    classical_defs = {\n",
    "        \"RandomForest\": (RandomForestClassifier(), {\"n_estimators\": [100, 200]}),\n",
    "        \"SVM\": (SVC(probability=True), {\"C\": [0.1, 1, 10], \"gamma\": [\"scale\"], \"kernel\": [\"rbf\"]}),\n",
    "        \"GB\": (GradientBoostingClassifier(), {\"n_estimators\": [100, 200], \"learning_rate\": [0.05, 0.1]})\n",
    "    }\n",
    "\n",
    "    train, test = process_data(X, y)\n",
    "    for name, (model, grid) in classical_defs.items():\n",
    "        gs = GridSearchCV(model, grid, cv=3, scoring=\"roc_auc\", n_jobs=-1)\n",
    "        gs.fit(train.X_scaled, train.y) # gs.fit(X_tr_s, y_tr)\n",
    "        best = gs.best_estimator_\n",
    "        print(f\"üîç Best {name} params:\", gs.best_params_)\n",
    "        best_params_all[name] = gs.best_params_\n",
    "\n",
    "        y_pred = best.predict(test.X_scaled) # best.predict(X_te_s)\n",
    "        y_prob = best.predict_proba(test.X_scaled)[:,1] # best.predict_proba(X_te_s)[:,1]\n",
    "        cm = confusion_matrix(test.y, y_pred) # confusion_matrix(y_te, y_pred)\n",
    "        confusion_matrices[name] = cm\n",
    "        roc_curves[name] = (test.y, y_prob) # (y_te, y_prob)\n",
    "        train_auc[name] = roc_auc_score(train.y, best.predict_proba(train.X_scaled)[:,1]) # roc_auc_score(y_tr, best.predict_proba(X_tr_s)[:,1])\n",
    "        test_auc[name] = roc_auc_score(test.y, y_prob) # roc_auc_score(y_te, y_prob)\n",
    "\n",
    "        df = pd.DataFrame(classification_report(y_te, y_pred, output_dict=True)).transpose(); df['model'] = name\n",
    "        summary.append(df)\n",
    "    \n",
    "    #Deep Learning Models with GridSearchCV\n",
    "    deep_defs = {\n",
    "        \"LSTM\": (build_lstm, {\n",
    "            \"units\": [32, 64],\n",
    "            \"learning_rate\": [0.001, 0.0005],\n",
    "            \"epochs\": [10],\n",
    "            \"batch_size\": [32]\n",
    "        }),\n",
    "        \"CNN\": (build_cnn, {\n",
    "            \"filters\": [16, 32],\n",
    "            \"kernel_size\": [3],\n",
    "            \"learning_rate\": [0.001],\n",
    "            \"epochs\": [10],\n",
    "            \"batch_size\": [32]\n",
    "        })\n",
    "    }\n",
    "\n",
    "    for name, (builder, grid) in deep_defs.items():\n",
    "        keras_clf = MyKerasClassifier(build_fn=builder)\n",
    "        gs = GridSearchCV(keras_clf, grid, cv=3, scoring=\"roc_auc\", n_jobs=1)\n",
    "        gs.fit(train.X, train.y) # gs.fit(X_tr_dl, y_tr_dl)\n",
    "\n",
    "        print(f\"üîç Best {name} params:\", gs.best_params_)\n",
    "        best_params_all[name] = gs.best_params_\n",
    "        best_dl = gs.best_estimator_\n",
    "\n",
    "        y_pred = best_dl.predict(test.X) # best_dl.predict(X_te_dl)\n",
    "        y_prob = best_dl.predict_proba(test.X) # best_dl.predict_proba(X_te_dl)\n",
    "        confusion_matrices[name] = confusion_matrix(test.y, y_pred) # confusion_matrix(y_te_dl, y_pred)\n",
    "        roc_curves[name] = (test.y, y_prob) # (y_te_dl, y_prob)\n",
    "        train_auc[name] = roc_auc_score(train.y, best_dl.predict_proba(train.X)) # roc_auc_score(y_tr_dl, best_dl.predict_proba(X_tr_dl))\n",
    "        test_auc[name] = roc_auc_score(test.y, y_prob) # roc_auc_score(y_te_dl, y_prob)\n",
    "\n",
    "        df = pd.DataFrame(classification_report(test.y, y_pred, output_dict=True)).transpose() # pd.DataFrame(classification_report(y_te_dl, y_pred, output_dict=True)).transpose()\n",
    "        df['model'] = name\n",
    "        summary.append(df)\n",
    "\n",
    "    #Ensemble Models\n",
    "    voting = VotingClassifier(estimators=[(k, v) for k, (v, _) in classical_defs.items()], voting='soft')\n",
    "    voting.fit(X_tr_s, y_tr)\n",
    "    y_prob = voting.predict_proba(X_te_s)[:,1]\n",
    "    y_pred = voting.predict(X_te_s)\n",
    "    confusion_matrices[\"Voting\"] = confusion_matrix(y_te, y_pred)\n",
    "    roc_curves[\"Voting\"] = (y_te, y_prob)\n",
    "    train_auc[\"Voting\"] = roc_auc_score(y_tr, voting.predict_proba(X_tr_s)[:,1])\n",
    "    test_auc[\"Voting\"] = roc_auc_score(y_te, y_prob)\n",
    "    df = pd.DataFrame(classification_report(y_te, y_pred, output_dict=True)).transpose(); df['model'] = \"Voting\"\n",
    "    summary.append(df)\n",
    "\n",
    "    stacking = StackingClassifier(estimators=[(k, v) for k, (v, _) in classical_defs.items()],\n",
    "                                   final_estimator=LogisticRegression())\n",
    "    stacking.fit(X_tr_s, y_tr)\n",
    "    y_prob = stacking.predict_proba(X_te_s)[:,1]\n",
    "    y_pred = stacking.predict(X_te_s)\n",
    "    confusion_matrices[\"Stacking\"] = confusion_matrix(y_te, y_pred)\n",
    "    roc_curves[\"Stacking\"] = (y_te, y_prob)\n",
    "    train_auc[\"Stacking\"] = roc_auc_score(y_tr, stacking.predict_proba(X_tr_s)[:,1])\n",
    "    test_auc[\"Stacking\"] = roc_auc_score(y_te, y_prob)\n",
    "    df = pd.DataFrame(classification_report(y_te, y_pred, output_dict=True)).transpose(); df['model'] = \"Stacking\"\n",
    "    summary.append(df)\n",
    "\n",
    "    #Save metrics and figures\n",
    "    pd.concat(summary).to_csv(os.path.join(RESULTS_DIR, \"summary_metrics.csv\"), index=False)\n",
    "    save_all_confusion_matrices(confusion_matrices)\n",
    "    save_all_roc_curves(roc_curves)\n",
    "    save_auc_comparison(train_auc, test_auc)\n",
    "    print(\"\\n‚úÖ Finished! All metrics and figures saved to:\", RESULTS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f0a4ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Finished! All metrics and figures saved to: ./results\n"
     ]
    }
   ],
   "source": [
    "#Save metrics and figures\n",
    "pd.concat(summary).to_csv(os.path.join(RESULTS_DIR, \"summary_metrics.csv\"), index=False)\n",
    "save_all_confusion_matrices(confusion_matrices)\n",
    "save_all_roc_curves(roc_curves)\n",
    "save_auc_comparison(train_auc, test_auc)\n",
    "print(\"\\n‚úÖ Finished! All metrics and figures saved to:\", RESULTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8f438a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "#Plotting Confusion Matrices\n",
    "fig, axs = plt.subplots(3, 3, figsize=(18, 15))\n",
    "axs = axs.flatten()\n",
    "for ax, (name, cm) in zip(axs, confusion_matrices.items()):\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=[\"Healthy\", \"Runaway\"],\n",
    "                yticklabels=[\"Healthy\", \"Runaway\"], ax=ax)\n",
    "    ax.set_title(f\"Confusion: {name}\")\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Actual\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#ROC Curves\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sorted_roc = sorted(roc_curves.items(), key=lambda x: auc(*roc_curve(x[1][0], x[1][1])[:2]), reverse=True)\n",
    "for name, (y_true, y_prob) in sorted_roc:\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    auc_val = auc(fpr, tpr)\n",
    "    ax.plot(fpr, tpr, label=f\"{name} (AUC={auc_val:.3f})\")\n",
    "ax.plot([0, 1], [0, 1], '--', color='gray')\n",
    "ax.set_title(\"ROC Curves (Sorted by AUC)\")\n",
    "ax.set_xlabel(\"False Positive Rate\")\n",
    "ax.set_ylabel(\"True Positive Rate\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Train vs Test AUC Comparison\n",
    "labels = list(train_auc.keys())\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(x - width/2, [train_auc[k] for k in labels], width, label=\"Train AUC\")\n",
    "plt.bar(x + width/2, [test_auc[k] for k in labels], width, label=\"Test AUC\")\n",
    "plt.xticks(x, labels, rotation=45)\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.title(\"Train vs Test AUC Comparison\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-fix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
